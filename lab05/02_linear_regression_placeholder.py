# encoding: utf-8
# çº¿æ€§å›å½’
# @version: v1.0.3 build 20180225
# å‚è€ƒ https://github.com/aymericdamien/TensorFlow-Examples/

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
tf.set_random_seed(777) # for reproducibility


# å®šä¹‰å‚æ•°
learning_rate   = 0.01
training_epochs = 2000
display_step    = 100

# è®­ç»ƒæ•°æ® æ¨¡æ‹Ÿç”ŸæˆXå’ŒYæ•°æ®, ä¸€é˜¶å¼ é‡: å‘é‡(vector)
train_x = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,
                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])
train_y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,
                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])
# è¾“å…¥å˜é‡å®šä¹‰çº¦å®š train_xs test_xs validation_xs batch_xs
# æˆ–è€…ä¸ç”¨åŠ sï¼Œä¹Ÿå¯ä»¥ï¼Œæ›´ç®€å•
n_samples = train_x.shape[0] # æ ·æœ¬æ•°
# print(n_samples)
''' 17 '''


# é€šè¿‡ y = W * x +b å»è¯•å›¾æŸ¥æ‰¾ W æƒé‡å’Œ b åå€¼ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬è¦æ±‚çš„å€¼
# æˆ‘ä»¬çŸ¥é“ W åº”è¯¥ä¸º 1 ï¼Œb åº”è¯¥ä¸º 0
# ä½†æ˜¯è¿™é‡Œæˆ‘ä»¬è®© Tensofrlow æ¥æŒ‡å‡º
W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name='Weight')
b = tf.Variable(tf.zeros([1]), name='bias')


# y_ å°±åƒè¾“å…¥å…‰æ ‡ä¸€æ ·ç”¨ä¸‹åˆ’çº¿æ¥ä»£è¡¨ï¼ŒğŸ‘
x  = tf.placeholder(tf.float32, name='x-input')
y_ = tf.placeholder(tf.float32, name='y-input')


# æ„å»ºä¸€ä¸ªçº¿æ€§æ¨¡å‹
# y = W * x + b
y = tf.add(tf.multiply(x, W), b)


# Mean squared error
# cost = tf.reduce_sum(tf.pow(y-y_, 2))/(2*n_samples)
# æˆæœ¬å‡½æ•° æœ€å°åŒ–æ–¹å·®
cost = tf.reduce_mean(tf.square(y - y_))


# Minimize
optimizer = tf.train.GradientDescentOptimizer(learning_rate)
train = optimizer.minimize(cost)


# å¯åŠ¨Graph
with tf.Session() as sess:

    # åˆå§‹åŒ–å˜é‡
    init = tf.global_variables_initializer()
    sess.run(init)

    # æ‹Ÿåˆå¹³é¢
    for epoch in range(training_epochs+1):
        sess.run(train, feed_dict={x: train_x, y_: train_y})

        if epoch % display_step == 0:
            loss = sess.run(cost, feed_dict={x: train_x, y_: train_y})
            print("Epoch: %04d cost=%.9f W=%s b=%s " % (epoch, loss, sess.run(W), sess.run(b)))

    print("Optimization Finished!")
    training_cost = sess.run(cost, feed_dict={x: train_x, y_: train_y})
    print("Training cost=%s, W=%s b=%s" % (training_cost, sess.run(W), sess.run(b)))
    # å¾—åˆ°æœ€ä½³æ‹Ÿåˆç»“æœ W: [1.00], b: [0.00]
    

    # æ‹Ÿåˆç»˜å›¾ï¼Œçº¿æ€§æ¨¡å‹
    plt.plot(train_x, train_y, 'ro', label='Original data')
    plt.plot(train_x, sess.run(W) * train_x + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()


    # æµ‹è¯•æ•°æ®
    test_x = np.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])
    test_y = np.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])
    loss = sess.run(cost, feed_dict={x: test_x, y_: test_y})
    print("Testing cost=%s" % loss)

    pred_y = sess.run(y, feed_dict={x: test_x})

    plt.plot(test_x, test_y, 'bo', label='Testing data')
    # plt.plot(test_x, sess.run(W) * test_x + sess.run(b), label='Fitted line')
    plt.plot(test_x, pred_y, label='Fitted line')
    plt.legend()
    plt.show()


    # ä½¿ç”¨å¾—åˆ°çš„ç»“æœè¿›è¡Œé¢„æµ‹
    print(sess.run(y, feed_dict={x: np.asarray([5])}))
    print(sess.run(y, feed_dict={x: np.asarray([2.5])}))



'''
Epoch: 0000 cost=0.830925822 W=[ 0.22327769] b=[ 0.15528001]
Epoch: 0100 cost=0.183793530 W=[ 0.32099679] b=[ 0.30705699]
Epoch: 0200 cost=0.172268465 W=[ 0.30603015] b=[ 0.41316369]
Epoch: 0300 cost=0.165180445 W=[ 0.29429296] b=[ 0.49637499]
Epoch: 0400 cost=0.160821259 W=[ 0.28508842] b=[ 0.5616312]
Epoch: 0500 cost=0.158140317 W=[ 0.27786994] b=[ 0.61280686]
Epoch: 0600 cost=0.156491548 W=[ 0.27220902] b=[ 0.65294015]
Epoch: 0700 cost=0.155477524 W=[ 0.26776963] b=[ 0.68441343]
Epoch: 0800 cost=0.154853895 W=[ 0.26428819] b=[ 0.70909542]
Epoch: 0900 cost=0.154470339 W=[ 0.26155794] b=[ 0.72845173]
Epoch: 1000 cost=0.154234484 W=[ 0.25941679] b=[ 0.74363148]
Epoch: 1100 cost=0.154089406 W=[ 0.25773764] b=[ 0.75553578]
Epoch: 1200 cost=0.154000223 W=[ 0.25642085] b=[ 0.76487118]
Epoch: 1300 cost=0.153945327 W=[ 0.25538817] b=[ 0.77219254]
Epoch: 1400 cost=0.153911591 W=[ 0.25457832] b=[ 0.77793384]
Epoch: 1500 cost=0.153890848 W=[ 0.2539432] b=[ 0.78243673]
Epoch: 1600 cost=0.153878078 W=[ 0.25344512] b=[ 0.78596777]
Epoch: 1700 cost=0.153870210 W=[ 0.25305453] b=[ 0.78873688]
Epoch: 1800 cost=0.153865367 W=[ 0.25274822] b=[ 0.79090852]
Epoch: 1900 cost=0.153862417 W=[ 0.25250801] b=[ 0.7926116]
Epoch: 2000 cost=0.153860584 W=[ 0.25231957] b=[ 0.79394746]
Optimization Finished!
Training cost=0.153861, W=[ 0.25231957] b=[ 0.79394746]
Testing cost=0.1563
[ 2.05554533]
[ 1.42474639]
'''
